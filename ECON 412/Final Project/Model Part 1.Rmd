---
title: "Model_Yiran"
author: 
- LIU, YIPING
- LATIFI, ROYA
- SUN, YIRAN
date: "`r format(Sys.Date(),'%b %d,%Y')`"
output: 
  rmarkdown::html_document:
    toc: true
    toc_depth: 3
    number_sections: false
    theme: readable
    df_print: paged
---
<style type="text/css">

h1.title {
  font-size: 38px;
  color: Black;
  text-align: center;
}

h4.author { /* Header 4 - and the author and data headers use this too  */
  font-size: 20px;
  color: Black;
  text-align: center;
}

h4.date { /* Header 4 - and the author and data headers use this too  */
  font-size: 18px;
  color: Gray;
  text-align: center;
}

body { 
  font-size: 14pt;
  lineheight: 14p;
}

</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE, include=FALSE, warning=FALSE, message=FALSE}
library(caret)
library(fastDummies)#create dummies
library(e1071)#cv
library(lares)
library(dplyr)
```


```{r,include= TRUE, echo =TRUE, warning = FALSE,message = FALSE}
## load dataset
da.train <- read.csv("train.csv")
da.test <- read.csv("test.csv")
dim(da.train)
dim(da.test)

all <- rbind(da.train,da.test) #combine train and test data 
head(all)

```

#### Split training and testing dataset
```{r,include= TRUE, echo =TRUE, warning = FALSE,message = FALSE}

set.seed(412)
train.index <- sample(c(1:dim(all)[1]), dim(all)[1]*0.8)

train.data <- all[train.index,]
valid.data <- all[-train.index, ] 

# check if training contains all the subject
da.train <- train.data
da.test <- valid.data

da.train$Partition = "Train"
da.test$Partition = "Test"

ind.all = rbind(da.train,da.test) #create another dataset for independent variable visualization 
# with test and train data partition
ind.all$Partition = as.factor(ind.all$Partition)
qplot(data = ind.all, x = subject, ylab="count",fill = Partition)

```

So train.data is our training data and valid.data is our testing data

<br>

# Naive Bayes

We directly use the original "Activity" Column instead of make it into one-vs-all problem

```{r,include= TRUE, echo =TRUE, warning = FALSE,message = FALSE}
set.seed(412) 
train.control <- trainControl(method = "cv", number = 5)

# Train the model
x.train <- train.data[,-563]
y.train <- factor(train.data$Activity)
x.valid <- valid.data[,-563]
y.valid <- factor(valid.data$Activity)

nb.model <- train(x=x.train, y=y.train, method = "nb",trControl = train.control)
print(nb.model)
```

### Predict on the testing data, Evaluate prediction result
```{r,include= TRUE, echo =TRUE, warning = FALSE,message = FALSE}
nb.pred <- predict(nb.model, x.valid)

# Evaluate with Confusion matrix
nb.matrix <- confusionMatrix(nb.pred, y.valid, positive="1", mode="everything")
print(nb.matrix)
```

<br><br>

# Random Forest

```{r,include= TRUE, echo =TRUE, warning = FALSE,message = FALSE}
set.seed(412) 

rf.model <- train(x=x.train, y=y.train, method = "rf",trControl = train.control)

print(rf.model)
```

### Predict on the testing data, Evaluate prediction result
```{r,include= TRUE, echo =TRUE, warning = FALSE,message = FALSE}
rf.pred <- predict(rf.model, x.valid)

# Evaluate with Confusion matrix
rf.matrix <- confusionMatrix(rf.pred, y.valid, positive="1", mode="everything")
print(rf.matrix)
```

Really great performance.

# LDA
```{r,include= TRUE, echo =TRUE, warning = FALSE,message = FALSE}
set.seed(412) 

lda.model <- train(x=x.train, y=y.train, method = "lda", trControl = train.control)

print(lda.model)

# prediction on validation dataset
lda.pred <- predict(lda.model, x.valid)

# Evaluate with Confusion matrix
lda.matrix <- confusionMatrix(lda.pred, y.valid, mode="everything")


print(lda.matrix)
```


# SVM 
```{r,include= TRUE, echo =TRUE, warning = FALSE,message = FALSE}
set.seed(412) 

svm.model <- train(x=x.train, y=y.train, method = "svmLinear2", trControl = train.control)

print(svm.model)

# prediction on validation dataset
svm.pred <- predict(svm.model, x.valid)

# Evaluate with Confusion matrix
svm.matrix <- confusionMatrix(svm.pred, y.valid, mode="everything")

print(svm.matrix)

```


# KNN

```{r,include= TRUE, echo =TRUE, warning = FALSE,message = FALSE}
cvBreaks = 5
temp = sample(levels(factor(train.data$subject)), length(levels(factor(train.data$subject))))  # randomize subjects
temp = split(temp, cut(1:length(temp), breaks = cvBreaks, labels = FALSE))  # split into CV groups
cvGroupIndices = lapply(temp, function(X) {
    which(!train.data$subject %in% X)
})

```

```{r,include= TRUE, echo =TRUE, warning = FALSE,message = FALSE}
set.seed(412) 
knnCtrl = trainControl(method = "cv", number = length(cvGroupIndices), index = cvGroupIndices, 
        classProbs = TRUE)

knn.model <- train(x=x.train, y=y.train, method = "knn", trControl = knnCtrl, 
                      tuneGrid = data.frame(.k = c(2,5, 10, 15, 20)))

print(knn.model)

# prediction on validation dataset
knn.pred <- predict(knn.model, x.valid)

# Evaluate with Confusion matrix
knn.matrix <- confusionMatrix(knn.pred, y.valid, mode="everything")

print(knn.matrix)
```

<br><br>


# After PCA Regularization

## Choose the number of components

```{r,include= TRUE, echo =TRUE, warning = FALSE,message = FALSE}
# Applying Principal Component Analysis to reduce number of dimensions

pc <- prcomp(x.train, center=TRUE, scale=TRUE)
pc.var <- pc$sdev^2
pc.pvar <- pc.var/sum(pc.var)

# Plotting Cumulative proportions of Principal Components to decide number of components 
plot(cumsum(pc.pvar),xlab="Principal component", 
     ylab="Cumulative Proportion of variance explained",
     type='b',main="Principal Components proportions",col="deepskyblue3")
points(x=100, y=cumsum(pc.pvar)[100],type="p", pch=21,cex=1.5,bg="black",col="black")
text(200,0.9,"Num of Comp=100, Var=0.946")

```


We would select 100 components.

## LDA with PCA preprocessing

```{r,include= TRUE, echo =TRUE, warning = FALSE,message = FALSE}
set.seed(412) 

lda.pca.model <- train(x=x.train, y=y.train, method = "lda",preProcess="pca",
                      pcaComp=100, trControl = train.control)

print(lda.pca.model)

# prediction on validation dataset
lda.pca.pred <- predict(lda.pca.model, x.valid)

# Evaluate with Confusion matrix
lda.pca.matrix <- confusionMatrix(lda.pca.pred, y.valid, mode="everything")

print(lda.pca.matrix)
```

<br><br>

## Naive Bayes with PCA preprocessing

```{r,include= TRUE, echo =TRUE, warning = FALSE,message = FALSE}
set.seed(412) 

nb.pca.model <- train(x=x.train, y=y.train, method = "nb",preProcess="pca",
                      pcaComp=100, trControl = train.control)
print(nb.pca.model)

# prediction on validation dataset
nb.pca.pred <- predict(nb.pca.model, x.valid)

# Evaluate with Confusion matrix
nb.pca.matrix <- confusionMatrix(nb.pca.pred, y.valid, positive="1", mode="everything")
print(nb.pca.matrix)
```

<br><br>

## RandomForest with PCA preprocessing

```{r,include= TRUE, echo =TRUE, warning = FALSE,message = FALSE}
set.seed(412) 

rf.pca.model <- train(x=x.train, y=y.train, method = "rf",preProcess="pca",
                      pcaComp=100, trControl = train.control)
print(rf.pca.model)


# prediction on validation dataset
rf.pca.pred <- predict(rf.pca.model, x.valid)

# Evaluate with Confusion matrix
rf.pca.matrix <- confusionMatrix(rf.pca.pred, y.valid, positive="1", mode="everything")
print(rf.pca.matrix)
```


<br><br>


## SVM with PCA preprocessing

```{r,include= TRUE, echo =TRUE, warning = FALSE,message = FALSE}
set.seed(412) 

svm.pca.model <- train(x=x.train, y=y.train, method = "svmLinear2",preProcess="pca",
                      pcaComp=100, trControl = train.control)

print(svm.pca.model)

# prediction on validation dataset
svm.pca.pred <- predict(svm.pca.model, x.valid)

# Evaluate with Confusion matrix
svm.pca.matrix <- confusionMatrix(svm.pca.pred, y.valid, mode="everything")

print(svm.pca.matrix)
```


## KNN with PCA preprocessing


```{r,include= TRUE, echo =TRUE, warning = FALSE,message = FALSE}
pca.x.train = data.frame(pc$x)
pca.x.train = pca.x.train[,1:100]

set.seed(412) 
knn.pca.model <- train(x=pca.x.train, y=y.train, method = "knn", 
                       trControl = train.control, 
                      tuneGrid = data.frame(.k = c(2,5, 10, 15, 20)))

print(knn.pca.model)

# prediction on validation dataset
pca.x.valid <- predict(pc, newdata = x.valid)
pca.x.valid <- as.data.frame(pca.x.valid)[,1:100]

knn.pca.pred <- predict(knn.pca.model, pca.x.valid)

# Evaluate with Confusion matrix
knn.pca.matrix <- confusionMatrix(knn.pca.pred, y.valid, mode="everything")

print(knn.pca.matrix)
```


































