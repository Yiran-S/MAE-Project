{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "from fake_useragent import UserAgent\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://www.reddit.com/r/wallstreetbets/\"\n",
    "\n",
    "ua = UserAgent()\n",
    "page = get(URL, headers={'User-Agent': ua.chrome})\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = soup.find_all('h3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_topics = [topic.text for topic in topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_topics = clean_topics.rename(columns={0:\"headlines\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A video of Russian police physically humiliati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ChaseApp - One Search. Any App.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Canada to require negative covid 19 test for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>China will seriously sanction’ any country tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The United Arab Emirates has become the fifth ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Syria 'finds body of archaeologist Khaled al-A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>YouTube removes Punjabi songs related to farme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  A video of Russian police physically humiliati...\n",
       "1                    ChaseApp - One Search. Any App.\n",
       "2  Canada to require negative covid 19 test for p...\n",
       "3  China will seriously sanction’ any country tha...\n",
       "4  The United Arab Emirates has become the fifth ...\n",
       "5  Syria 'finds body of archaeologist Khaled al-A...\n",
       "6  YouTube removes Punjabi songs related to farme..."
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_topics.to_csv(\"Headlines of Worldnews_YiranSun.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load scrape_reddit2.py\n",
    "from scrape_reddit1 import lovely_soup\n",
    "import pandas as pd\n",
    "\n",
    "URL = \"https://www.reddit.com/r/wallstreetbets/\"\n",
    "\n",
    "soup = lovely_soup(URL)\n",
    "\n",
    "titles = soup.find_all('h3')\n",
    "headlines= pd.DataFrame([title.text for title in titles])\n",
    "\n",
    "links = soup.find_all(\"a\")\n",
    "\n",
    "\n",
    "#----------\n",
    "def scrape(URL):\n",
    "    ua = UserAgent()\n",
    "\n",
    "    soup = lovely_soup(URL)\n",
    "\n",
    "    comments = soup.find_all('p')\n",
    "\n",
    "    import re\n",
    "    pattern = re.compile(r' [A-Z]{3} ')\n",
    "\n",
    "    clean_comments = [pattern.findall(comment.text) for comment in comments]\n",
    "\n",
    "    ticks = []\n",
    "    for i in clean_comments:\n",
    "        if i != []:\n",
    "            ticks += i\n",
    "    return(clean_comments)\n",
    "\n",
    "\n",
    "url_list = []\n",
    "for i in links:\n",
    "    href = i.attrs[\"href\"]\n",
    "    if href.startswith('/r/wallstreetbets'):\n",
    "        url_list.append(URL[:-18] + href)\n",
    "\n",
    "#from scrape_reddit1 import scrape\n",
    "\n",
    "our_crawl = []\n",
    "\n",
    "for u in url_list:\n",
    "    df = scrape(u)\n",
    "    print(df)\n",
    "    our_crawl += df\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "from fake_useragent import UserAgent\n",
    "import pandas as pd\n",
    "\n",
    "URL = \"https://www.reddit.com/r/wallstreetbets/comments/l84ner/for_those_who_have_been_around_for_a_while_what/\"\n",
    "\n",
    "\n",
    "\n",
    "ua = UserAgent()\n",
    "\n",
    "def lovely_soup(u):\n",
    "    page = get(u, headers={'User-Agent': ua.chrome})\n",
    "    return(BeautifulSoup(page.content, 'html.parser'))\n",
    "\n",
    "soup = lovely_soup(URL)\n",
    "\n",
    "comments = soup.find_all('p')\n",
    "\n",
    "clean_comments = [comment.text for comment in comments]\n",
    "\n",
    "\n",
    "clean_comments  = pd.DataFrame(clean_comments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
