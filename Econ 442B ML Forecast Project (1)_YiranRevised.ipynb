{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Econ-442B-Machine-Learning-Forecast-Project\" data-toc-modified-id=\"Econ-442B-Machine-Learning-Forecast-Project-0.1\"><center>Econ 442B Machine Learning Forecast Project</center></a></span></li></ul></li><li><span><a href=\"#Creating-Dataset\" data-toc-modified-id=\"Creating-Dataset-1\"><center>Creating Dataset</center></a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Forecasting-SPY\" data-toc-modified-id=\"Forecasting-SPY-1.0.1\"><center>Forecasting SPY</center></a></span></li><li><span><a href=\"#Using-the-following-Predictors:\" data-toc-modified-id=\"Using-the-following-Predictors:-1.0.2\"><center>Using the following Predictors:</center></a></span></li><li><span><a href=\"#Dollar-Index-(Dixie)\" data-toc-modified-id=\"Dollar-Index-(Dixie)-1.0.3\"><center>Dollar Index (Dixie)</center></a></span></li><li><span><a href=\"#VIX\" data-toc-modified-id=\"VIX-1.0.4\"><center>VIX</center></a></span></li><li><span><a href=\"#Breakeven-Inflation\" data-toc-modified-id=\"Breakeven-Inflation-1.0.5\"><center>Breakeven Inflation</center></a></span></li><li><span><a href=\"#Federal-Funds-Rate\" data-toc-modified-id=\"Federal-Funds-Rate-1.0.6\"><center>Federal Funds Rate</center></a></span></li></ul></li><li><span><a href=\"#Target-Function-of-Choice-is:-Sharpe-Ratio\" data-toc-modified-id=\"Target-Function-of-Choice-is:-Sharpe-Ratio-1.1\"><center>Target Function of Choice is: Sharpe Ratio</center></a></span></li><li><span><a href=\"#-Split-Data-Into-Three-Parts\" data-toc-modified-id=\"-Split-Data-Into-Three-Parts-1.2\"><center> Split Data Into Three Parts</center></a></span></li><li><span><a href=\"#--Produce-a-Heat-Map-of-Hyperparameters\" data-toc-modified-id=\"--Produce-a-Heat-Map-of-Hyperparameters-1.3\"><center>  Produce a Heat Map of Hyperparameters</center></a></span></li><li><span><a href=\"#-Evaluate-model-performance-for-the-⅔-of-the-data-choose-optimal-parameters/hyper-parameters-using-⅔-of-data\" data-toc-modified-id=\"-Evaluate-model-performance-for-the-⅔-of-the-data-choose-optimal-parameters/hyper-parameters-using-⅔-of-data-1.4\"><center> Evaluate model performance for the ⅔ of the data choose optimal parameters/hyper-parameters using ⅔ of data</center></a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Econ 442B Machine Learning Forecast Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Creating Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Forecasting SPY "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Using the following Predictors:\n",
    "### <center>Dollar Index (Dixie)\n",
    "### <center>VIX\n",
    "### <center>Breakeven Inflation\n",
    "### <center>Federal Funds Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Necessary Packages and establish working directory\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.stats import skew\n",
    "from statistics import mean\n",
    "from statistics import stdev\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import numpy as np \n",
    "from numpy.random import seed\n",
    "import scipy.stats as st\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "import random, heapq\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>SPY</th>\n",
       "      <th>DIXIE</th>\n",
       "      <th>VIX</th>\n",
       "      <th>BREAK</th>\n",
       "      <th>RATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003-01-01</td>\n",
       "      <td>60.379791</td>\n",
       "      <td>4.10</td>\n",
       "      <td>31.170000</td>\n",
       "      <td>1.76</td>\n",
       "      <td>1.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003-02-01</td>\n",
       "      <td>59.565929</td>\n",
       "      <td>3.87</td>\n",
       "      <td>29.629999</td>\n",
       "      <td>1.91</td>\n",
       "      <td>1.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003-03-01</td>\n",
       "      <td>59.453651</td>\n",
       "      <td>2.99</td>\n",
       "      <td>29.150000</td>\n",
       "      <td>1.87</td>\n",
       "      <td>1.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003-04-01</td>\n",
       "      <td>64.744141</td>\n",
       "      <td>2.99</td>\n",
       "      <td>21.209999</td>\n",
       "      <td>1.78</td>\n",
       "      <td>1.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003-05-01</td>\n",
       "      <td>68.294456</td>\n",
       "      <td>3.25</td>\n",
       "      <td>19.469999</td>\n",
       "      <td>1.66</td>\n",
       "      <td>1.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date        SPY  DIXIE        VIX  BREAK  RATE\n",
       "0 2003-01-01  60.379791   4.10  31.170000   1.76  1.24\n",
       "1 2003-02-01  59.565929   3.87  29.629999   1.91  1.26\n",
       "2 2003-03-01  59.453651   2.99  29.150000   1.87  1.25\n",
       "3 2003-04-01  64.744141   2.99  21.209999   1.78  1.26\n",
       "4 2003-05-01  68.294456   3.25  19.469999   1.66  1.26"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading dataset\n",
    "# Read the required data to prep for analysis\n",
    "Data = pd.ExcelFile('Dataset_442B.xlsx')\n",
    "Data= pd.read_excel(Data)\n",
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SPY_Change</th>\n",
       "      <th>DIXIE</th>\n",
       "      <th>VIX</th>\n",
       "      <th>BREAK</th>\n",
       "      <th>RATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2003-02-01</th>\n",
       "      <td>-0.013479</td>\n",
       "      <td>3.87</td>\n",
       "      <td>29.629999</td>\n",
       "      <td>1.91</td>\n",
       "      <td>1.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-03-01</th>\n",
       "      <td>-0.001885</td>\n",
       "      <td>2.99</td>\n",
       "      <td>29.150000</td>\n",
       "      <td>1.87</td>\n",
       "      <td>1.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-04-01</th>\n",
       "      <td>0.088985</td>\n",
       "      <td>2.99</td>\n",
       "      <td>21.209999</td>\n",
       "      <td>1.78</td>\n",
       "      <td>1.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-05-01</th>\n",
       "      <td>0.054836</td>\n",
       "      <td>3.25</td>\n",
       "      <td>19.469999</td>\n",
       "      <td>1.66</td>\n",
       "      <td>1.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-06-01</th>\n",
       "      <td>0.007014</td>\n",
       "      <td>3.44</td>\n",
       "      <td>19.520000</td>\n",
       "      <td>1.61</td>\n",
       "      <td>1.22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            SPY_Change  DIXIE        VIX  BREAK  RATE\n",
       "2003-02-01   -0.013479   3.87  29.629999   1.91  1.26\n",
       "2003-03-01   -0.001885   2.99  29.150000   1.87  1.25\n",
       "2003-04-01    0.088985   2.99  21.209999   1.78  1.26\n",
       "2003-05-01    0.054836   3.25  19.469999   1.66  1.26\n",
       "2003-06-01    0.007014   3.44  19.520000   1.61  1.22"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Changing variables to represent returns\n",
    "Data[\"SPY_Change\"] = Data[\"SPY\"].pct_change()\n",
    "Data.dropna(inplace=True)\n",
    "\n",
    "# Dropped SPY\n",
    "Data = Data.drop(columns=[\"SPY\"])\n",
    "\n",
    "# Change Index to Datetime\n",
    "datetime_series = pd.to_datetime(Data['Date'])\n",
    "datetime_index = pd.DatetimeIndex(datetime_series.values)\n",
    "Data = Data.set_index(datetime_index)\n",
    "Data = Data.drop(columns=[\"Date\"])\n",
    "\n",
    "# Reordering columns\n",
    "Data = pd.DataFrame(Data, columns = [\"SPY_Change\", \"DIXIE\", \"VIX\", \"BREAK\",\"RATE\"])\n",
    "Data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Target Function of Choice is: Sharpe Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sharpe ratio was developed by Nobel laureate William F. Sharpe and is used to help investors understand the return of an investment compared to its risk.The ratio is the average return earned in excess of the risk-free rate per unit of volatility or total risk. Volatility is a measure of the price fluctuations of an asset or portfolio.\n",
    "\n",
    "Generally, the greater the value of the Sharpe ratio, the more attractive the risk-adjusted return.\n",
    "\n",
    "The greater a portfolio's Sharpe ratio, the better its risk-adjusted-performance. If the analysis results in a negative Sharpe ratio, it either means the risk-free rate is greater than the portfolio’s return, or the portfolio's return is expected to be negative. In either case, a negative Sharpe ratio does not convey any useful meaning.\n",
    "\n",
    "Sharpe Ratios above 1.00 are generally considered “good”, as this would suggest that the portfolio is offering excess returns relative to its volatility. Having said that, investors will often compare the Sharpe Ratio of a portfolio relative to its peers. Therefore, a portfolio with a Sharpe Ratio of 1.00 might be considered inadequate if the competitors in its peer group have an average Sharpe Ratio above 1.00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Function: Sharpe Ratio\n",
    "\n",
    "def sharpe_ratio(returns, rfr):\n",
    "    excess_returns = returns - rfr\n",
    "    n = len(excess_returns)\n",
    "    mean = excess_returns.mean()\n",
    "    std = np.sqrt(np.sum(np.square(excess_returns-mean))/(n-1))\n",
    "    return(mean/std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: RNN\n",
    "\n",
    "def sequentialize(asset):\n",
    "    d = asset.copy()\n",
    "    X = d.iloc[:,1:]\n",
    "    Y = d.iloc[:,0]\n",
    "\n",
    "    input_sequences = []\n",
    "    y = []\n",
    "    obs = d.shape[0]\n",
    "\n",
    "    for i in range(WINDOW, obs):\n",
    "        input_sequences.append(X.iloc[i-WINDOW:i,:])\n",
    "        y.append(Y.iloc[i])\n",
    "\n",
    "    input_sequences = np.array(input_sequences)\n",
    "    y = np.array(y)\n",
    "    return(input_sequences, y)\n",
    "\n",
    "def my_RNN(asset, WINDOW, LAYERS):\n",
    "    d = asset.copy()\n",
    "    (input_sequences, y) = sequentialize(d)\n",
    "    \n",
    "    rnn = Sequential()\n",
    "\n",
    "    rnn.add(LSTM(units = 30, return_sequences=True, \n",
    "                 input_shape =  input_sequences[0].shape))\n",
    "    rnn.add(Dropout(.2))\n",
    "\n",
    "    for i in range(LAYERS):\n",
    "        rnn.add(LSTM(units = 30, return_sequences=True))\n",
    "        rnn.add(Dropout(.2))\n",
    "\n",
    "    rnn.add(LSTM(units = 30, return_sequences=False))\n",
    "\n",
    "    rnn.add(Dense(units = 1))\n",
    "\n",
    "    rnn.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "    rnn.fit(input_sequences, y, epochs = 15, batch_size = 32)\n",
    "    return(rnn)\n",
    "\n",
    "def evaluate(asset, rnn, input_sequences, WINDOW):\n",
    "    d = asset.copy()\n",
    "    pred = rnn.predict(input_sequences)\n",
    "    asset_c = d.iloc[WINDOW:,:]\n",
    "    asset_c[\"Preds\"] = pred\n",
    "    asset_c[\"Returns\"] = 0.\n",
    "    asset_c[\"Returns\"][asset_c[\"Preds\"] > 0] = asset_c[\"SPY_Change\"]\n",
    "    asset_c[\"Returns\"][asset_c[\"Preds\"] < 0] = -asset_c[\"SPY_Change\"]\n",
    "    return(asset_c[[\"Returns\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "7/7 [==============================] - 5s 35ms/step - loss: 0.0086\n",
      "Epoch 2/15\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0027\n",
      "Epoch 3/15\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.0027\n",
      "Epoch 4/15\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.0024\n",
      "Epoch 5/15\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.0023\n",
      "Epoch 6/15\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.0021\n",
      "Epoch 7/15\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.0019\n",
      "Epoch 8/15\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.0022\n",
      "Epoch 9/15\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.0020\n",
      "Epoch 10/15\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.0021\n",
      "Epoch 11/15\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 0.0016\n",
      "Epoch 12/15\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0017\n",
      "Epoch 13/15\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.0019\n",
      "Epoch 14/15\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0019\n",
      "Epoch 15/15\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.0018\n"
     ]
    }
   ],
   "source": [
    "# Set random state\n",
    "random.seed(442)\n",
    "seed(442)\n",
    "tf.random.set_seed(442)\n",
    "\n",
    "WINDOW = 20\n",
    "LAYERS = 1\n",
    "asset = Data.copy()\n",
    "\n",
    "rnn = my_RNN(asset, WINDOW, LAYERS)\n",
    "(input_sequences, y ) = sequentialize(asset)\n",
    "R = evaluate(asset, rnn, input_sequences, WINDOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Returns    0.092324\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate Sharpe Ratio based on a risk free rate of 1%\n",
    "monthly_risk_free_rate = 1.01**(1/12)-1\n",
    "sharpe_ratio(pd.DataFrame(R),monthly_risk_free_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "So we can see here, with WINDOW=20 and LAYERS=1, the Sharpe Ratio for our asset is 0.092324. And now we will split out data into three parts and apply different sets of hyperparameters and choose the best one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Split Data Into Three Parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = len(Data) // 3\n",
    "Data_sect1 = Data.iloc[:split,:]\n",
    "Data_sect2 = Data.iloc[split:2*split,:]\n",
    "Data_sect3 = Data.iloc[2*split:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>  Produce a Heat Map of Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = [10,20,30,40,50] # WINDOW\n",
    "hs = [1,2,3]  # LAYERS\n",
    "\n",
    "# Initialize Data frame to record results for heatmap\n",
    "evaluation_Sharpe = pd.DataFrame(np.zeros((len(ts), len(hs))), index = ts, columns = hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "2/2 [==============================] - 4s 19ms/step - loss: 0.0063\n",
      "Epoch 2/15\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0033\n",
      "Epoch 3/15\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0034\n",
      "Epoch 4/15\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0021\n",
      "Epoch 5/15\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0018\n",
      "Epoch 6/15\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0018\n",
      "Epoch 7/15\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0016\n",
      "Epoch 8/15\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0020\n",
      "Epoch 9/15\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0017\n",
      "Epoch 10/15\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0016\n",
      "Epoch 11/15\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0012\n",
      "Epoch 12/15\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0011\n",
      "Epoch 13/15\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0012\n",
      "Epoch 14/15\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0016\n",
      "Epoch 15/15\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0015\n",
      "Epoch 1/15\n",
      "2/2 [==============================] - 6s 25ms/step - loss: 0.0018\n",
      "Epoch 2/15\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0016\n",
      "Epoch 3/15\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0017\n",
      "Epoch 4/15\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0014\n",
      "Epoch 5/15\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0012\n",
      "Epoch 6/15\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0017\n",
      "Epoch 7/15\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0014\n",
      "Epoch 8/15\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0015\n",
      "Epoch 9/15\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0013\n",
      "Epoch 10/15\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0013\n",
      "Epoch 11/15\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 9.6052e-04\n",
      "Epoch 12/15\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 9.9307e-04\n",
      "Epoch 13/15\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0012\n",
      "Epoch 14/15\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0012\n",
      "Epoch 15/15\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0011\n",
      "Epoch 1/15\n",
      "2/2 [==============================] - 7s 32ms/step - loss: 0.0015\n",
      "Epoch 2/15\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0014\n",
      "Epoch 3/15\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0015\n",
      "Epoch 4/15\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0013\n",
      "Epoch 5/15\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0013\n",
      "Epoch 6/15\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0015\n",
      "Epoch 7/15\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0012\n",
      "Epoch 8/15\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0013\n",
      "Epoch 9/15\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0012\n",
      "Epoch 10/15\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0010\n",
      "Epoch 11/15\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 9.7463e-04\n",
      "Epoch 12/15\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0011\n",
      "Epoch 13/15\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0011\n",
      "Epoch 14/15\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0011\n",
      "Epoch 15/15\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0012\n",
      "Epoch 1/15\n",
      "2/2 [==============================] - 5s 35ms/step - loss: 0.0100\n",
      "Epoch 2/15\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0069\n",
      "Epoch 3/15\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0029\n",
      "Epoch 4/15\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0038\n",
      "Epoch 5/15\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0027\n",
      "Epoch 6/15\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0027\n",
      "Epoch 7/15\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0023\n",
      "Epoch 8/15\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0016\n",
      "Epoch 9/15\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0017\n",
      "Epoch 10/15\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0027\n",
      "Epoch 11/15\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0012\n",
      "Epoch 12/15\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0016\n",
      "Epoch 13/15\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0022\n",
      "Epoch 14/15\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0013\n",
      "Epoch 15/15\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0016\n",
      "WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f934434d790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/15\n",
      "2/2 [==============================] - 6s 48ms/step - loss: 0.0030\n",
      "Epoch 2/15\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0018\n",
      "Epoch 3/15\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0023\n",
      "Epoch 4/15\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0014\n",
      "Epoch 5/15\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0018\n",
      "Epoch 6/15\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0014\n",
      "Epoch 7/15\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0012\n",
      "Epoch 8/15\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0015\n",
      "Epoch 9/15\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0011\n",
      "Epoch 10/15\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0015\n",
      "Epoch 11/15\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0014\n",
      "Epoch 12/15\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0011\n",
      "Epoch 13/15\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0010\n",
      "Epoch 14/15\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0011\n",
      "Epoch 15/15\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0013\n",
      "WARNING:tensorflow:6 out of the last 16 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f934ef9b3a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/15\n",
      "2/2 [==============================] - 8s 58ms/step - loss: 0.0025\n",
      "Epoch 2/15\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.0024\n",
      "Epoch 3/15\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.0020\n",
      "Epoch 4/15\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.0015\n",
      "Epoch 5/15\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.0012\n",
      "Epoch 6/15\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.0012\n",
      "Epoch 7/15\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.0016\n",
      "Epoch 8/15\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0012\n",
      "Epoch 9/15\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.0015\n",
      "Epoch 10/15\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.0014\n",
      "Epoch 11/15\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0013\n",
      "Epoch 12/15\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0010\n",
      "Epoch 13/15\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.0012\n",
      "Epoch 14/15\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.0011\n",
      "Epoch 15/15\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0013\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f9356e9cdc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/15\n",
      "2/2 [==============================] - 6s 78ms/step - loss: 0.0122\n",
      "Epoch 2/15\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.0045\n",
      "Epoch 3/15\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.0028\n",
      "Epoch 4/15\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0018\n",
      "Epoch 5/15\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.0023\n",
      "Epoch 6/15\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.0030\n",
      "Epoch 7/15\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0026\n",
      "Epoch 8/15\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0024\n",
      "Epoch 9/15\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0016\n",
      "Epoch 10/15\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0028\n",
      "Epoch 11/15\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0022\n",
      "Epoch 12/15\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0020\n",
      "Epoch 13/15\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0015\n",
      "Epoch 14/15\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0017\n",
      "Epoch 15/15\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0019\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f9344059940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/15\n",
      "2/2 [==============================] - 7s 70ms/step - loss: 0.0048\n",
      "Epoch 2/15\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.0028\n",
      "Epoch 3/15\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0021\n",
      "Epoch 4/15\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.0019\n",
      "Epoch 5/15\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.0017\n",
      "Epoch 6/15\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.0016\n",
      "Epoch 7/15\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.0020\n",
      "Epoch 8/15\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.0016\n",
      "Epoch 9/15\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.0021\n",
      "Epoch 10/15\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.0015\n",
      "Epoch 11/15\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.0013\n",
      "Epoch 12/15\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.0012\n",
      "Epoch 13/15\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.0014\n",
      "Epoch 14/15\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.0014\n",
      "Epoch 15/15\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.0016\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f935b357af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/15\n",
      "2/2 [==============================] - 8s 83ms/step - loss: 0.0038\n",
      "Epoch 2/15\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.0020\n",
      "Epoch 3/15\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.0038\n",
      "Epoch 4/15\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.0026\n",
      "Epoch 5/15\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0017\n",
      "Epoch 6/15\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.0023\n",
      "Epoch 7/15\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.0018\n",
      "Epoch 8/15\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.0016\n",
      "Epoch 9/15\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0018\n",
      "Epoch 10/15\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.0016\n",
      "Epoch 11/15\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0013\n",
      "Epoch 12/15\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0014\n",
      "Epoch 13/15\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0015\n",
      "Epoch 14/15\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.0016\n",
      "Epoch 15/15\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.0015\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f9341516b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/15\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.0111\n",
      "Epoch 2/15\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0068\n",
      "Epoch 3/15\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0060\n",
      "Epoch 4/15\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0035\n",
      "Epoch 5/15\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0046\n",
      "Epoch 6/15\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0043\n",
      "Epoch 7/15\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0029\n",
      "Epoch 8/15\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0034\n",
      "Epoch 9/15\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0027\n",
      "Epoch 10/15\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0024\n",
      "Epoch 11/15\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0024\n",
      "Epoch 12/15\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0023\n",
      "Epoch 13/15\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0024\n",
      "Epoch 14/15\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0028\n",
      "Epoch 15/15\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0026\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f9357d78670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.0058\n",
      "Epoch 2/15\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0041\n",
      "Epoch 3/15\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0048\n",
      "Epoch 4/15\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0028\n",
      "Epoch 5/15\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0021\n",
      "Epoch 6/15\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0023\n",
      "Epoch 7/15\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0033\n",
      "Epoch 8/15\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0023\n",
      "Epoch 9/15\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0025\n",
      "Epoch 10/15\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0018\n",
      "Epoch 11/15\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0019\n",
      "Epoch 12/15\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0017\n",
      "Epoch 13/15\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0017\n",
      "Epoch 14/15\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0022\n",
      "Epoch 15/15\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0022\n",
      "WARNING:tensorflow:7 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f936d0aa3a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/15\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.0019\n",
      "Epoch 2/15\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0106\n",
      "Epoch 3/15\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0023\n",
      "Epoch 4/15\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0030\n",
      "Epoch 5/15\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0049\n",
      "Epoch 6/15\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0041\n",
      "Epoch 7/15\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0029\n",
      "Epoch 8/15\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0019\n",
      "Epoch 9/15\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0017\n",
      "Epoch 10/15\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0025\n",
      "Epoch 11/15\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0026\n",
      "Epoch 12/15\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0030\n",
      "Epoch 13/15\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0021\n",
      "Epoch 14/15\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0020\n",
      "Epoch 15/15\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0018\n",
      "WARNING:tensorflow:7 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f9357d78820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/15\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.0088\n",
      "Epoch 2/15\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0085\n",
      "Epoch 3/15\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0042\n",
      "Epoch 4/15\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0047\n",
      "Epoch 5/15\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0034\n",
      "Epoch 6/15\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0031\n",
      "Epoch 7/15\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0047\n",
      "Epoch 8/15\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0020\n",
      "Epoch 9/15\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0025\n",
      "Epoch 10/15\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0037\n",
      "Epoch 11/15\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0033\n",
      "Epoch 12/15\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0022\n",
      "Epoch 13/15\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0017\n",
      "Epoch 14/15\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0036\n",
      "Epoch 15/15\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.002 - 0s 66ms/step - loss: 0.0025\n",
      "WARNING:tensorflow:8 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f9341527430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/15\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.0052\n",
      "Epoch 2/15\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0033\n",
      "Epoch 3/15\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0051\n",
      "Epoch 4/15\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0044\n",
      "Epoch 5/15\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0029\n",
      "Epoch 6/15\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0031\n",
      "Epoch 7/15\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0025\n",
      "Epoch 8/15\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0027\n",
      "Epoch 9/15\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0025\n",
      "Epoch 10/15\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0022\n",
      "Epoch 11/15\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0023\n",
      "Epoch 12/15\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0021\n",
      "Epoch 13/15\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0027\n",
      "Epoch 14/15\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0025\n",
      "Epoch 15/15\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0029\n",
      "WARNING:tensorflow:8 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f934ef9bb80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/15\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.0037\n",
      "Epoch 2/15\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0094\n",
      "Epoch 3/15\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0035\n",
      "Epoch 4/15\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0039\n",
      "Epoch 5/15\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0067\n",
      "Epoch 6/15\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0048\n",
      "Epoch 7/15\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0024\n",
      "Epoch 8/15\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0020\n",
      "Epoch 9/15\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0022\n",
      "Epoch 10/15\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0026\n",
      "Epoch 11/15\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0029\n",
      "Epoch 12/15\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0034\n",
      "Epoch 13/15\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0035\n",
      "Epoch 14/15\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0024\n",
      "Epoch 15/15\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0024\n",
      "WARNING:tensorflow:9 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f93573993a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "# Loop over hyperparameters and record Sharpe ratio\n",
    "for n, WINDOW in enumerate(ts):\n",
    "    for m , LAYERS in enumerate(hs):\n",
    "        asset = Data_sect1.copy()\n",
    "        \n",
    "        # Set random state\n",
    "        random.seed(442)\n",
    "        seed(442)\n",
    "        tf.random.set_seed(442)\n",
    "        \n",
    "        rnn = my_RNN(asset, WINDOW, LAYERS)\n",
    "        (input_sequences, y ) = sequentialize(asset)\n",
    "        R = evaluate(asset, rnn, input_sequences, WINDOW)\n",
    "        Sharpe = sharpe_ratio(pd.DataFrame(R), monthly_risk_free_rate)[0]\n",
    "        evaluation_Sharpe.iloc[n,m] = Sharpe    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiQAAAEvCAYAAAB13NouAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAA0L0lEQVR4nO3dd3yV9fn/8dd1TsJegUDCFBAVt7LU4gDrQGsFa1G+rlZrcVSrtdbR+WurFmrtcBWptWpbS7WtigoiDkBRhKAoQxAEhBBCIGGTQHLO9fsjh5g9kJP7nOT99HE/uMfnc+7r6C258lm3uTsiIiIiQQoFHYCIiIiIEhIREREJnBISERERCZwSEhEREQmcEhIREREJnBISERERCVxKvG8wK2Os5hXLQTW9ddwfW2lGTi8MOgJpir626V/WmPcr3rK6wT9rU9P71xmjmY0C/gSEgcfdfUI1ZUYAfwRSgS3ufkZ965anv9lFRESkCjMLA48AZwPZwAIzm+ruy8qV6QQ8Coxy93Vm1q2+dStTl42IiEiyi0YavtVtGLDK3Ve7+z5gCjC6UpnLgP+5+zoAd89rQN0KlJCIiIgkO482fKtbT2B9uePs2LnyDgfSzGyWmS00s6saULcCddmIiIgku2i9EowKzGw8ML7cqcnuPrl8kWqqVR6rkgIMBr4KtAbeM7N59axb5YNEREQkiXn9Wjwq1fHJwORaimQDvcsd9wJyqimzxd13A7vNbA5wfD3rVqAuGxERkWQXjTZ8q9sC4DAz62dmLYBxwNRKZV4ETjOzFDNrA5wEfFLPuhWohURERCTZHUALSZ0f6V5iZjcBMyiduvuEuy81s+tj1ye5+ydm9irwMRCldHrvEoDq6tZ2PyUkIiIiya5+s2YazN2nAdMqnZtU6fh+4P761K2NEhIREZFkF4cWksamhERERCTZHcAsm0SjhERERCTJHcgsm0SjhERERCTZqYVEREREAqcWEhEREQlcnGbZNCYlJCIiIslOLSQiIiISOI0hERERkcA1gRYSvctGREREAqcWEhERkWSnLhsREREJmrtm2YiIiEjQmsAYEiUkjaDzyBMYcM/VWDjExn++wbqHXqhwvcuoIfS7cxxEHS+JsOpnT7J9/nIAen73fHpc8VXA2PjP18meXO8XJ0oTdvgZx3Phz6/CwiEW/PstZv15aoXrR509mHNuuwT3KNGSKC/96mnWZq2gY/fOXPr7G2nftRMedd7/1xvM/durAX0LSSRdRx7PUfeUPlPr//kWnz1U8ZnKGDWYw++8BI9G8ZIoy372NFvnrwCg33Xn0fuyMwFnxyfr+fiWSUT3FgfwLZoxddlInUIhDpvwHT665NfszSlg8IzfsGVGFns+zS4rsm3OErJevR2Atkf14ejJtzH/1FtpO7A3Pa74KgtH3Y3vK+G4KT8hf+YHFK7JDerbSAKwkDHmV1fz+BX3sT03n5um3suymQvJW7WhrMyquUtYNnMhAJkD+3D5I9/nga/eTrQkysv3/IOcpWtp0bYV33/pPla+vbhCXWmGQsbRE67m/Uvuoygnn1Nn3MumGQvZ9ekXz8WWOUvY9GrpM9X+qD4Mmvx9Zp96Oy0z0+h77Shmn3Y70aJiTpx8Cz3GnEL2v+cE9W2apybQQqJZNnHWYdAACtfkUvR5Hl5cQt4Lc0kfNaRCmcieorL9cJtWuDsAbQ7ryY6FK4kW7sMjUba9u4z084c1avySeHqfMID8z3MpWJ9HpDjCRy+9x1HnVHym9u3ZW7bfok1LKH2k2Ll5GzlL15aW2V1E3mcb6JjZubFClwTVadAA9qzJpfDzPLw4Qs4L75FR5e+pL56pcLlnCsDCYcKtWmDhEOE2LSjK3dpYoct+0UjDtwSjFpI4a5nZmb05+WXHe3MK6DDosCrl0s8bRv+fXEZqekcWX/EbAHYvX0+/u/+PlLR2RIv20fmsQez86LNGi10SU8eMNLaVe6a2b8ynzwkDqpQ7+twhjLpjHO26dORv1/y2yvW0Xun0PKov6xatimu8kvhaZaZRWO6ZKsrJp9Ogqs9UxnlDGPiTcbRI78iCK0qfqb25W1n955c584OHiRTuY8vsj9kye3GjxS4xTaCFRAlJvFnVU17+V4uYLdPns2X6fDqefCT97ryUj8b+mj0rN7Du4Rc5/tmfEdldxO6la/GSxMtqpZFZ1YfKqz5SLJ2RxdIZWfQbNpBzbhvL41fcV3atRZuWXPHnHzD1V0+zd1dhPKOVZFDNM1WdTdOz2DQ9i84nD+SIO8fy/tj7SOnYloxRQ3hr6Pcp3r6HQY/fQs+LT2XDf9+Jc9BSQRMYQ1Jrl42ZdTSzCWa23MzyY9snsXOdaqk33syyzCzrpcLVBz3oZLJ3YwEte3QpO27ZozP7cgtqLL993ie06ptJauf2AOQ+8yYLz76TRWN+QfG2XRSu3hj3mCWxbc8toFO5Z6pj9y7syKu5iXzN/OV0OSSDNmmlz1QoJcyVk37AohfmsnTGgrjHK4mvaGMBrcs9U616dKm126Vg3nLa9M0gtXN70k8/hsJ1eezL34mXRMh9ZQFpQw9vjLClPI82fEswdY0heRbYCoxw9y7u3gUYGTv3XE2V3H2yuw9x9yFfb93/4EWbhHZ+uIrW/bvTqk83LDWFbmOGs2VGVoUyrftmlu23O7YfodQUigt2ApCa3gGAlj3T6Xr+SeQ9P7fxgpeElP3RZ3Tpm0lar66EU8Mc//VT+CQ2gHW/LodklO33OLov4dQU9mwtfaa+OXE8eatyePuvmrElpbZ/+Blt+2fSuk9XLDVMjzGnsGlGxWeqTd8vnqkOx/Yt+3uqaMMWOg06jFDrFgCkn3YMu1ZqkHSji0YbviWYurps+rr7xPIn3D0XmGhm18QvrKbDI1FW3v1Xjpvyk9Jpv/96iz0rsulx1dkA5Dw9k/QLTiJz7Bl4SYRI0T6Wjf9DWf2j/3o7qWnt8ZISPr37cUq27w7qq0iCiEaivPjzJ/nO03cTCodY8OwsNq3M5qTLzwLg/X++zjHnDWPwN04nUlJCcdE+nrnpQQD6DjmCwRefzsZP1nHLtNKxSq/+9t+smLUoqK8jCcAjUZbc/STDptyNhUNk/2sWu1Zk0+eq0mdq3dOvk3nBMHqNPZ1oSQnRon18ML70mdr2wWdsfPl9Tpt5Hx6Jsn3xWtb9/Y0gv07zlIAJRkOZV9f5vP+i2WvA68BT7r4pdi4D+DZwtrufVdcNZmWMrfkGIgdgemsNfZKD53QNoZE4+Nqmf9VvYM5BUjjnyQb/rG19+rcbNca61NVlcynQBZhtZgVmVgDMAjoDY+Mcm4iIiNRHU++ycfetwJ2xrQIzuxr4W5ziEhERkfpKwEGqDfVlFkb75UGLQkRERA5cU28hMbOPa7oEZNRwTURERBpTnFpIzGwU8CcgDDzu7hMqXR8BvAisiZ36n7v/KnZtLbATiAAl7l5x+d9K6hodmAGcS+k03woxAO/WUVdERESSlJmFgUeAs4FsYIGZTXX3ZZWKvu3uF9TwMSPdfUt97ldXQvIy0M7dF1UT6Kz63EBERETiLD5dMMOAVe6+GsDMpgCjgcoJyUFR6xgSd/+Ou1e7/q+7XxaPgERERKSB4rNSa09gfbnj7Ni5yk4xs4/MbLqZHV0+KuA1M1toZuPrupkWdBAREUl2B9BCEksSyicKk919cvki1VSrvN7JB8Ah7r7LzM4HXgD2v0F2uLvnmFk3YKaZLXf3OTXFo4REREQk2R1AQhJLPibXUiQb6F3uuBeQU+kzdpTbn2Zmj5pZurtvcfec2Pk8M3ue0i6gGhOSLzPtV0RERBJBfLpsFgCHmVk/M2sBjAOmli9gZplmpa+LNrNhlOYV+WbW1szax863Bc4BltR2M7WQiIiIJLs4DGp19xIzuwmYQem03yfcfamZXR+7Pgn4JnCDmZUAhcA4d/fYa2aej+UqKcAz7v5qbfdTQiIiIpLs4rQOibtPA6ZVOjep3P7DwMPV1FsNHN+QeykhERERSXYJuPJqQykhERERSXZN4F02SkhERESSnVpIREREJHBKSERERCRwXnm9suSjhERERCTZqYVEREREAqeERERERAKnWTYiIiISuCbQQqJ32YiIiEjg1EIiIiKS7DTLpm5nbX033reQZmbbNScHHYI0Ie88kRp0CCJfXhPoslELiYiISLJTQiIiIiKB0ywbERERCZpHNYZEREREgqYuGxEREQmcumxEREQkcOqyERERkcCpy0ZEREQCp4REREREAqeVWkVERCRwaiERERGRwGlQq4iIiARO035FREQkcE2ghSQUdAAiIiLy5Xg02uCtPsxslJmtMLNVZnZXNddHmNl2M1sU235e37qVqYVEREREqjCzMPAIcDaQDSwws6nuvqxS0bfd/YIDrFtGLSQiIiLJLuoN3+o2DFjl7qvdfR8wBRhdz4gaXFcJiYiISLLzaIM3MxtvZlnltvGVPrUnsL7ccXbsXGWnmNlHZjbdzI5uYN0y6rIRERFJdgcwqNXdJwOTayli1VWrdPwBcIi77zKz84EXgMPqWbcCtZCIiIgku2i04VvdsoHe5Y57ATnlC7j7DnffFdufBqSaWXp96lamhERERCTZxWcMyQLgMDPrZ2YtgHHA1PIFzCzTzCy2P4zSvCK/PnUrU5eNiIhIsovDwmjuXmJmNwEzgDDwhLsvNbPrY9cnAd8EbjCzEqAQGOfuDlRbt7b7KSERERFJdnFaGC3WDTOt0rlJ5fYfBh6ub93aqMumEZx7zgiWLpnD8mXvcMePvlfl+v/930V8sHAmHyycyduzX+S4446qd11pnsKHn0Cb2x6kze0Pk3rGRTWWC/U6lLb3Pkv4mJPLzqV+5Wu0vuUPtL71j6QO/1pjhCtJIH3k8Zw29/ecNu+P9Lv5wirXu40azPC3JvKVNyZwyox76TTsiLJrh1x3PsNn38/w2fdz/KSbCbVMbczQhfgtjNaYlJDEWSgU4sE/3csFX7+CY48fyaWXjuHIIw+rUGbtmvWc+dVvMmjw2dx73x+Z9OjEeteVZshCtLzwuxT+7V72/OFWUo4/FevWq9pyLUZdSWTlR2WnQhm9SRl6FoWP3knhg7cRHjgE69K9EYOXhBQyjppwDVmXTeCd035I94uG0/bwijM08+csYe7IO3n3q3ex+AePcczvS2eItsxM45BrR/HeuT9m7hk/glCI7mO+EsS3aN7iM4akUSkhibNhQ0/ks8/WsmbNOoqLi3n22Re58OvnVijz3rwstm3bDsC89z+gZ8/u9a4rzU+o9wCi+bn41k0QKaHko3dIOXJolXKpXzmPyJJ5+K7tZeesay+i6z+F4n0QjRJZs5SUo4c1ZviSgDoNGsCeNbkUfp6HF0fIfeFdMkYNqVAmsmdv2X5Km5YVJnBaOEy4VQssHCLcpiVFuVsbK3TZr6knJGbW0cwmmNlyM8uPbZ/EznVqpBiTWo+emazP/mKmU/aGjfTokVlj+WuuHserM946oLrSPFiHzvj2LWXHvqMA69ilSpmUo06i+P3XKpyPblpHuN9R0KYdpLYg5YhBWMf0RolbElfLzM4U5uSXHRflFNAys3OVct3OG8qp7zzAoH/cyZIflA4j2Ju7lbV/fpkzPniEkR9PomTHHvJnf9xosUvMASyMlmjqGtT6LPAmMMLdc6F0ig/wLeA5Steol1rEZkNVUDoAuaoRZ3yFq6/+P84YcVGD60pzUs16Q5Wei5YXXM3eV/9e5S8d37yBfbNfoPU1v4B9RUQ3roVoJI6xSlKobgmratawypu+gLzpC0g7eSAD7ryErLH3ktKxLd1GDWb20Jsp2b6HEx6/le4Xn8rG/74T97ClnARs8WiouhKSvu4+sfyJWGIy0cyuqalSbPnZ8QAW7kgo1PZLB5qsNmRvpHevHmXHvXp2Z+PGTVXKHXvskTw26X4uuPBKCgq2NqiuNC++I79Cq4Z16IzvKKhQJtTzUFr9322l19u0J3zEIPZGo0SWzack6w1Kst4AoMU5lxHdkY80b3s3FtC6xxetbK16dGZvLd0uW+ctp03fDFI7t6fz8KMoXLeZ4vydAGx6ZT5pQw9XQtLIvAkkJHWNIfnczO4ws4z9J8wsw8zupOIa9RW4+2R3H+LuQ5pzMgKwIGsRAwb0o2/f3qSmpnLJJaN56eWKzei9e/fguX//hW9ffQsrV65uUF1pfqLZqwild8fSukE4hZTjTyXySVaFMnvuv5E9v72BPb+9gZIl89j74mQiy+YDYG07lP7ZMZ2Uo0+mZJF+cDR32z/8jDb9M2ndpyuWGiZzzFfIm7GwQpk2fct+DNDh2L6EUlMoLthJ0YZ8Og4aQKh1CwC6nHYMu1ZuaNT4hSYxhqSuFpJLgbuA2bGkxIFNlK62dkmcY2sSIpEIt9z6U6a98gzhUIgnn/o3y5Z9yvjvXgnA5L/8nZ/+5Ad06ZLGQw/dB0BJSQknn3J+jXWlmYtG2Tv1cVpf8zOwEMVZbxLNW0/KsHMAKJlfe9La6vIfYW3a49EIe6f+BYp2N0bUksA8EmXZ3X9jyJQfY+EQ2f96i10rsul91VkArH/6dTIuOIkeY0/DSyJEi/axaPyfANj+wSo2vfw+X5n5GzwSZcfitaz/+xtBfp3mKQGn8TaU1TUmwcwGUroG/bz969XHzo9y91frukFKi56Jl4ZJUtv2w5PrLiRST+88oTUz5OAbtWlKtSNz4mXnjec1+Gdt+0enN2qMdalrls33gReBm4AlZja63OX74hmYiIiI1FMz6LL5LjA49lrhvsB/zKyvu/+JGsZli4iIiDRUXQlJuNxrhdea2QhKk5JDUEIiIiKSEJrCkhB1zbLJNbMT9h/EkpMLgHTg2DjGJSIiIvXVDLpsrgJKyp9w9xLgKjN7LG5RiYiISP0lYILRULUmJO6eXcu1uQc/HBEREWmoprAwWl0tJCIiIpLolJCIiIhI4JJ/XTQlJCIiIslOXTYiIiISPCUkIiIiEjh12YiIiEjQ1GUjIiIiwVMLiYiIiARNLSQiIiISvCbQQlLXu2xEREQkwXm04Vt9mNkoM1thZqvM7K5ayg01s4iZfbPcubVmttjMFplZVl33UguJiIhIsotDC4mZhYFHgLOBbGCBmU1192XVlJsIzKjmY0a6+5b63E8tJCIiIkkuTi0kw4BV7r7a3fcBU4DR1ZS7GfgvkPdlvoMSEhEREalOT2B9uePs2LkyZtYTuAiYVE19B14zs4VmNr6um6nLRkREJNkdQJdNLEkonyhMdvfJ5YtUU63ydJ4/Ane6e8SsSvHh7p5jZt2AmWa23N3n1BSPEhIREZEkV99BqhXqlCYfk2spkg30LnfcC8ipVGYIMCWWjKQD55tZibu/4O45sfvkmdnzlHYBKSERERFpqg4kIamHBcBhZtYP2ACMAy6rcF/3fvv3zexJ4GV3f8HM2gIhd98Z2z8H+FVtN4t7QvJA5sh430KamT3z19ddSKSeTjk/NegQRL60eCQk7l5iZjdROnsmDDzh7kvN7PrY9erGjeyXATwfazlJAZ5x91dru59aSERERJKdVzfc4yB8rPs0YFqlc9UmIu7+7XL7q4HjG3IvJSQiIiJJLk5dNo1KCYmIiEiS82h8WkgakxISERGRJKcWEhEREQmcx2kMSWNSQiIiIpLk1EIiIiIigdMYEhEREQmcV17QPQkpIREREUlyaiERERGRwCkhERERkcCpy0ZEREQC1xRaSEJBByAiIiKiFhIREZEkp4XRREREJHBaGE1EREQCF1ULiYiIiARNXTYiIiISuKYwy0YJiYiISJLTOiQiIiISOLWQiIiISOA0qFVEREQCp0GtUi+HnHEcZ/y/K7FwiKVTZpH16EsVrvc/exCn3P5NPOpEIxHm/PIf5Cz4FICr5/6BfbuL8EiUaCTClAt+HsRXkATTYugw2n3vZgiFKJr2CnumPFNtuZQjBpL20KPsuOeX7J0zG4Au/5yC7ynEoxGIRNh643WNGbokqJRjhtLqshshFKJ4znT2TptSbblwvyNo+9MH2fPneyjJeptQZi/a3PDTsuuhrt0pev4p9s38X2OFLmgMidSDhYwR93yL5y+fwK6NBYx76VesnrmQgpU5ZWXWz13K6pkfAJA+sDfnPXozfz/zjrLr/730Xoq27mr02CVBhUK0//6tbL3jh0Q3bybt0cfY+95cIp9/XqVcu+9ex76sBVU+YusPb8V3bG+kgCXhWYhWV97M7t/diRdspt3PH6F40btEc9ZVLTf2WkqWZJWdiuZms+sX15ddb/+HKRR/8E4jBi/QNLps9C6bOMs44VC2r93EjnWbiRZH+PSlefQ/Z3CFMsV79pbtp7Rp2TRSXYmblIFHUrJhA9GNG6GkhL1vvUnLr5xapVzrMd9g79uziW7bGkCUkkzC/Y8gmpeDb94IkRKK588i9cThVcq1OGsMxVlv4zu2Vfs5KUedWPo5+Xlxjlgqc7cGb4lGCUmctctMY2dOQdnxro0FtMtIq1Lu0HOHcOWbv2X0k7cz80d/KTvv7lz0j7sY98qvOeaykY0SsyS2cHo60c1f/IUf3byZUHp6hTKh9HRannoahS9NrfoBDp1++zvS/jyZVl/7erzDlSRgael4QblnqmAzltalYplOXUgdNJx9b71c4+eknjSS4vffilucUjP3hm+JptYuGzPrCNwNjAG6xk7nAS8CE9x9WzyDaxKsahZa3YPw2YwsPpuRRY9hR3DK7d/k+csmAPDcxb9i96ZttO7SgYv+eScFq3LImb8i3lFLQqvmN5tKz1S7G29m118eg2jVF1xsveV7RPPzsU6d6PTbB4is+5zixR/HKVZJDnU/U60vu5Gi5x6v+aUp4RRSTjiFov88fvDDkzrFq8vGzEYBfwLCwOPuPqGGckOBecCl7v6fhtTdr64xJM8CbwIj3D03doNM4FvAc8DZNQQ2HhgPcEnaML7S7rA6btN07dpYQPsencuO23XvzO68mpvQc+avoGOfbrRKa0fR1l3s3rQNgML8HXw2YyGZJxyqhKSZi2zZTKhrt7LjUNeuRPO3VCiTevgRdPxp6QBo69iRlsNOxiMR9s19h2h+PgC+bRv73nmblIFHKiFp5nzrZqxzuWeqc1d8W36FMuG+h9Pmhp8AYO06knLcMAojEUo+fBeAlOOGEfl8ZY3dORJf8eiCMbMw8AilP+uzgQVmNtXdl1VTbiIwo6F1y6ury6avu0/cn4wAuHuuu08E+tRUyd0nu/sQdx/SnJMRgE0fraZTv0w69O5KKDXM4V8/uWwA634dD8ko2+96TF/CLVIo2rqLlNYtSW3bCoCU1i3pc9ox5K/IbtT4JfGULF9OSs9ehDIzISWFliPPZO+7cyuUyb9iHPmXl25758xm54N/YN/cd6BVK6x169JCrVrRYshQImvXBPAtJJFE1qwg3K0nlp4J4RRSh42gOJZo7LfzjivZ+aMr2PmjKyjOmkPh3x8sS0ZA3TVBi7o1eKuHYcAqd1/t7vuAKcDoasrdDPyX0h6UhtYtU1cLyedmdgfwlLtvAjCzDODbwPp6fJlmzyNRZv3sKcb8/Q4sHGLZv2dT8OkGjr3iTAAW/+NNBpw/lCMvPpVocYSSon1M/97DALTp2oELJt8KQCglzIoX3uXz2fpNttmLRtj50B/pNPF3WChE4fRpRD5fS6sLLgSg6OVqxo3EhNLS6PjLewCwcJiiN15n34L5jRK2JLBolMJ/PkTbH04onfb79qtEcz6nxYgLANg3q+ZxIwC0aEnK0YMpfOqP8Y9VGlNPKv6szwZOKl/AzHoCFwFnAkMbUrcy81pGtphZGnAXpVlNBqW9ipuAqcBEdy+osXLMn/pckYBDZySZ/d9hyoXl4GnZJzXoEKQJ6vi31xt1Gsu8Ht9o8M/aUzY+fx2x4RUxk9198v4DMxsLnOvu18aOrwSGufvN5co8Bzzg7vPM7EngZXf/T33qVlZrC4m7bzWz/wL/cfcFZnY0MAr4pD7JiIiIiMTfgQxqjSUfk2spkg30LnfcC8ipVGYIMMVKJ3CkA+ebWUk961ZQ1yybXwDnASlmNpPSPqHZwF1mdqK731tbfREREYm/OK0rsgA4zMz6ARuAccBlFe/r/fbvl2shecHMUuqqW1ldY0i+CZwAtARygV7uvsPM7gfeB5SQiIiIBKyGydhfiruXmNlNlM6eCQNPuPtSM7s+dn1SQ+vWdr+6EpISd48Ae8zsM3ffEbtRoZnF4/uLiIhIA3l1a8kcjM91nwZMq3Su2kTE3b9dV93a1JWQ7DOzNu6+Byhb7zy2YJoSEhERkQQQbQLTR+pKSE53970A7hWW50uldHE0ERERCVg0Ti0kjamuWTZ7azi/BdhS3TURERFpXPHqsmlMdbWQiIiISIJrCmMolJCIiIgkObWQiIiISODUQiIiIiKBU0IiIiIigVOXjYiIiAQumvz5iBISERGRZNfk1yERERGRxNcEFmolFHQAIiIiImohERERSXKaZSMiIiKBi5rGkIiIiEjAmsIYEiUkIiIiSU5dNiIiIhI4rUMiIiIigdM6JCIiIhI4jSGph0lFn8b7FtLMXPfbHwcdgjQhvuqjoEMQ+dLUZSMiIiKB06BWERERCZy6bERERCRw6rIRERGRwKnLRkRERALXFBISve1XREQkybk1fKsPMxtlZivMbJWZ3VXN9dFm9rGZLTKzLDM7tdy1tWa2eP+1uu6lFhIREZEkF48WEjMLA48AZwPZwAIzm+ruy8oVewOY6u5uZscBzwIDy10f6e5b6nM/tZCIiIgkuegBbPUwDFjl7qvdfR8wBRhdvoC773L3/ZN82vIlJvwoIREREUlyfgCbmY2PdbPs38ZX+tiewPpyx9mxcxWY2UVmthx4BbimUlivmdnCaj67CnXZiIiINEPuPhmYXEuR6kaaVGkBcffngefN7HTg18BZsUvD3T3HzLoBM81subvPqelmaiERERFJclFr+FYP2UDvcse9gJyaCseSjUPNLD12nBP7Mw94ntIuoBopIREREUlycRpDsgA4zMz6mVkLYBwwtXwBMxtgZhbbHwS0APLNrK2ZtY+dbwucAyyp7WbqshEREUly8Zhl4+4lZnYTMAMIA0+4+1Izuz52fRJwMXCVmRUDhcClsRk3GZR240BprvGMu79a2/2UkIiIiCS5eL3Lxt2nAdMqnZtUbn8iMLGaequB4xtyLyUkIiIiSU7vshEREZHANYWl45WQiIiIJLl4ddk0JiUkIiIiSS7aBFISJSQiIiJJTl02IiIiErjkbx9RQiIiIpL01EIi9XLqyJP58b0/JBQO8Z9/vMjjDz1d4foFF5/LtTdfBcCe3YX88o6JrFi6EoDXs15g9649RKJRIiURxp7zrUaPXxLP3I9WMPHpqUSjzkUjh/KdC0dWuL5g2Wfc+sBT9OzWGYAzhx7D9d84i7U5m7njoX+WlcvOK+DGb57NFeed1qjxS+KZuzKH305bSNSdiwYdyjWnH13h+oI1m/jBM3PokdYWgK8e2ZvrRh5bdj0SjXLZpBl069Cah64Y0ZihC5r2K/UQCoX42cQ7+M7Ym9iUk8ezrz3FWzPe5rNP15SVyV6Xw1Wjr2fH9p2cduYp/PJ3dzPuvC9emPitb9zAtoLtQYQvCSgSjXLf317gsbuvJaNLRy776cOMGHQUh/bKqFDuxIH9ePhHV1c417dHV579za1ln3P29+7lzCHHNFbokqAi0Si/eTmLSd86k4wOrbn8sRmcMbAXh3brWKHciYd0rTHZeOa9FfTr2oHde4sbIWKprCkMatW7bOLsuEFHs25NNtmf51BcXMK051/jzFGnVyizaMFidmzfCcBHC5eQ2aNbEKFKkliyaj29M7rQK6MLqSkpjDrleGYtXNbgz3l/ySp6Z3ShR9e0OEQpyWRJdj69O7ejV+d2pKaEOffYQ5i1PLve9Tdt38Pbn+bwjcGHxjFKqY0fwJZolJDEWbfMruRu2FR2vGljHhndu9ZY/uLLL+TtN94rO3aHvz77EP+Z+RRjrxwTz1AlSeRt3U5ml05lx906d2RTNS1oH69cx9i7/siNE//KquzcKtdffe8jRp1yQhwjlWSRt7OQzI5ty44zOrQhb8eeKuU+Xr+FSx6ZxveefotVedvKzt8/fSG3nnsisfeWSADi9HK9RlVrl42ZdQTuBsYA+3+K5gEvAhPcfVs8g2sKqvsf1GtITYcNH8zFl13IFV8fX3busguuZfOmLXROT+Ovzz3MmpWfkzXvw3iFK0mguuen8nN2ZN+evPrgXbRp1ZK3P1zODx54mpf+cEfZ9eKSEmYvXMYt40bFO1xJAvV6prp3Zvpto2nTMpW3P93AD56Zw0u3XsicFRtIa9uKo3p0ZsGaTVU/SBpFc+iyeRbYCoxw9y7u3gUYGTv3XE2VzGy8mWWZWda2wryDF20S2rQxj8yeX/TtZ3TvRl7u5irlDj9qAL/+w0+46aofsW3rF7/tbt60BYCCLVt5fdosjh10VPyDloSW0bkjufnbyo7zCrbTLa1DhTLt2rSiTauWAJx24kBKIlG27thddv2dRSsY2K8nXTq2b5SYJbFldGhN7vYvno9NO/bQtX3rCmXatUqlTctUAE47vCclUWfr7iIWrdvM7BXZnPf7F7nrubksWLOJH//n3UaNX5qGuhKSvu4+0d3L2nvdPTf2dr8+NVVy98nuPsTdh3Rq3bzHQyz+cBmH9O9Nzz49SE1N4fyLzuGtGW9XKNO9ZwYP/m0id37vF6xdva7sfOs2rWjTtk3Z/vARJ7Hyk88aNX5JPEcf2ot1uflk5xVQXFLCq+99xBmDj6xQZsu2nXjs197Fq9YT9Sid2rcpuz793UWcd0qDXsQpTdjRPbuwrmAnG7buorgkwozFn3PGwJ4VymzZWfjFM5W9BXenU5uWfP/sE3jt9ouYfttoJowdztB+Gdz3za8E8TWataYwhqSuWTafm9kdwFPuvgnAzDKAbwPr4xxbkxCJRLjnrvt5/N8PEgqH+N8zL7FqxWou/dY3APj3U//jxh9eS6e0jvx84p2ldWLTe7t07cxDT94PQEo4zMv/m8E7b80L7LtIYkgJh7n726O5YcJfiUajjBkxlAG9Mnn29dJn45KzTmbm+4t59vX3SAmHadkihYk3X1bWBF+4dx/zlqziZ9d+I8ivIQkkJRzirq8N4Yan3yIadUYP6s+Abp14bkHp8gNjhx7G68vW8ez8VaSEjJapYSaMHa4xIwkkEceENJR5TQMaADNLA+4CRgP7+x1ygZcoHUNSUNcNjuw2LBETMUliH07/cdAhSBPiqz4KOgRpglpf+otGzdZu6zuuwT9rf792SkJllLV22bj7Vne/090Hunuau6cBWe5+R32SEREREYm/Jt9lY2ZTqzl95v7z7n5hXKISERGRemsKXTZ1jSHpBSwDHqc0oTJgKPBAnOMSERGRevKEbPNomLpm2QwBFgI/Aba7+yyg0N1nu/vseAcnIiIidWvyC6O5exT4g5k9F/tzU111REREpHE1hYXR6pVcuHs2MNbMvgbsiG9IIiIi0hDJn440sLXD3V8BXolTLCIiInIAmk0LiYiIiCSuRBwT0lB626+IiEiS8wP4pz7MbJSZrTCzVWZ2VzXXR5vZx2a2KPYOu1PrW7cytZCIiIgkuXi0kJhZGHgEOBvIBhaY2VR3X1au2BvAVHd3MzuO0pfyDqxn3QrUQiIiIpLk4tRCMgxY5e6r3X0fMIXSV8l8cV/3Xf7FO2ja8sX42jrrVqaEREREJMnFaR2SnlR8kW527FwFZnaRmS2ndNLLNQ2pW54SEhERkSQXdW/wZmbjY+M+9m/jK31sdS/fq9K04u7Pu/tAYAzw64bULU9jSERERJohd58MTK6lSDbQu9xxLyCnls+bY2aHmll6Q+uCWkhERESSXpze9rsAOMzM+plZC2AcUOGlu2Y2wMwstj8IaAHk16duZWohERERSXLxWBjN3UvM7CZgBhAGnnD3pWZ2fez6JOBi4CozKwYKgUtjg1yrrVvb/ZSQiIiIJLl4ve3X3acB0yqdm1RufyIwsb51a6OEREREJMk1hZValZCIiIgkOb3LRkRERAIXry6bxqSEREREJMmpy0ZEREQC98Xq7clLCYmIiEiS0xiSepgQGhDvW0gzE539StAhSBOSetXdQYcg8qWpy0ZEREQCp0GtIiIiEjh12YiIiEjgNKhVREREAqcxJCIiIhI4jSERERGRwDWFMSShoAMQERERUQuJiIhIktOgVhEREQlcU+iyUUIiIiKS5DSoVURERAIXVZeNiIiIBC350xElJCIiIklPY0hEREQkcEpIREREJHCa9isiIiKBUwuJiIiIBE7TfkVERCRwTaHLRu+yERERSXJRvMFbfZjZKDNbYWarzOyuaq5fbmYfx7Z3zez4ctfWmtliM1tkZll13UstJCIiIkkuHi0kZhYGHgHOBrKBBWY21d2XlSu2BjjD3bea2XnAZOCkctdHuvuW+txPCYmIiEiSi9Og1mHAKndfDWBmU4DRQFlC4u7vlis/D+h1oDdTl42IiEiS8wP4px56AuvLHWfHztXkO8D0CmHBa2a20MzG13UztZA0gm4jj+PYX18F4RDr/vkWKx9+qcL1zHMHM/DOsRCN4pEoi3/2dwrmrwCg//jzOOTykeDOjk/W8+GtjxHdWxzE15AEEup7NC1GjINQiJLFb1Oy4NWK13sdTsvR38O35wNQsuoDSua9DECLc75FuP9x+J6dFD39/xo5cklU78zLYsIfJxGJRrn466O49spLqpSZ/8HHTPzTY5SUlJDWqQNPPnI/AD+97/fMmTufzmmdeOEfkxo7dOHA3mUTSxLKJwqT3X1y+SLVVKv2RmY2ktKE5NRyp4e7e46ZdQNmmtlyd59TUzxKSOItZBz3m6t595LfULgxnzNevYfc1z5g56cbyopsfnsJuTMWAtDhyN4MmXwLb552O60y0+h/7bm8efqPiBYVM2Ty9+k55hTW/7vG/57SHJjR4szL2PvfP+A7t9Lq8p8Q+ewjvGBjhWLRDavY+8JDVaqXLH2X4kVv0XLUNY0VsSS4SCTCPQ88wl/+eB+Z3dK59NpbGHnqSRza75CyMjt27uKeBx7msQfuoXtmN/K3biu7Nub8s7ns4gv58a9/F0D0cqBiycfkWopkA73LHfcCcioXMrPjgMeB89w9v9zn58T+zDOz5yntAqrxB5i6bOIs7cQB7F6ziT3r8vDiCBteeI/McwdXKBPZs7dsP9ymFZTLdEPhMOFWLbBwiHDrFhTlbm202CUxhTL74ds249u3QDRCyfIFhA89od71oxtWQtHu+AUoSWfxJ5/Sp1cPevfsTmpqKud99QzefHtehTLTZs7irDOG0z2zGwBd0jqVXRtywrF07NC+MUOWSuLUZbMAOMzM+plZC2AcMLV8ATPrA/wPuNLdPy13vq2Ztd+/D5wDLKntZmohibNW3dMozClLGCncWEDaoAFVynU/bwhH/ngcLdM7MO+K0mbQotytrPrzK5yz8CEiRfvIm7WYzbMXN1rskpisXSd8Z0HZse/aSqh7vyrlQt370+rKn+O7trFvzn/w/Cq/2IgAkLd5C5ndupYdZ3RLZ/HSFRXKrF2XTUkkwrdvuoM9ewq5fOxoRp93VmOHKjU4kC6burh7iZndBMwAwsAT7r7UzK6PXZ8E/BzoAjxqZgAl7j4EyACej51LAZ5x91eruU2ZWhMSM+sI3A2MAfY/rXnAi8AEd992AN+xWYn9x6iomgdn4/QsNk7PosvJAznyzrG8e8l9pHZsS+aowcwcdgvF2/cw9C+30Ovi4WT/d24jRC6Jq7pnquJhNG8dhY/fBcV7CfU7hpYX3kjR337aOOFJ0qnuZ1nlv7oikSjLlq/k8QcnsHfvXi6/7jaOP3ogffsc8KQKOYjitVKru08DplU6N6nc/rXAtdXUWw0cX/l8berqsnkW2AqMcPcu7t4FGBk791xNlcxsvJllmVnWjD2rGhJPk1OYU0DrHl3Kjlt371xrt0v+vOW06duNFp3b0/X0Y9izLo99+Tvxkggbpy2g89DDGyNsSWC+ayvWvnPZsbVLw3dtq1hoXxEUl3YFRtcsgVAYWrVrxCglmWR0Syc3b3PZ8aa8LXRN71KlzPCTh9CmdSvSOnVk8AnHsGLVmsYOVWoQdW/wlmjqSkj6uvtEd8/df8Ldc919ItCnpkruPtndh7j7kHPbVO2eaE62LfqMtv0zadOnK5YapueYU8h9bWGFMm37ZpTtdzy2L6HUFPYV7KQwewtpgw8j3LoFAOmnHc3OlRuQ5i2auxbr1A3rkA6hMCkDhxJZ/VHFQm06lO2GMvuW/rpbtKtxA5WkcczAw1mXnUN2Ti7FxcVMf2M2I089uUKZkaedzAcfLaGkJEJhURGLl66gf9/eNXyiNLY4jSFpVHWNIfnczO4AnnL3TQBmlgF8m4pzk6UGHony8Y+f5JR/3YWFQ6z71yx2rthA36u+CsDap9+g+wXD6D32NLy4hEhRMVnXlc6M2PrhZ+S8/D5nvHYfHomwffFaPv/7m0F+HUkEHmXfW8/Q8uJbwYySJXPx/BxSjjsDgJKPZ5Ny+GBSjhsBHsFLitn3yl/Kqrc4/7uEex0OrdvR6ru/pfi9qUSWvBPMd5GEkJIS5sc/uIHrbvspkUiEiy44hwH9D+Hfz78CwKUXfY1D+/Zh+ElD+Ma3biBkIS7++rkc1r8vAD/6xQQWfPgx27bt4KtjruDG71zJxV8/N8Bv1PwkYotHQ1lty82aWRpwF6Urs2VQ2lO9idJRthPdvaDGyjEvZl6W/P+WJKGcfUfboEOQJiT1qruDDkGaoNT0/tWt4RE3/dNPbPDP2tVbPmzUGOtSawuJu28F7oxtmNlplM4jXlyfZERERETizz0adAhfWq1jSMxsfrn9a4EHgXbAL6p765+IiIg0vni97bcx1TWGJLXc/nXAOe6+2cx+R+lLdCbELTIRERGpl3i87bex1ZWQhGLjSEKUjjfZDODuu82sJO7RiYiISJ0SscWjoepKSDoCCyldicnNLNPdc82sHdW/dEdEREQaWZNvIXH3vjVcigIXHfRoREREpMGawrTfA3qXjbvvAbREn4iISAJIxIXOGkov1xMREUlyTb7LRkRERBJfcxjUKiIiIgmuKbSQ1PVyPREREZG4UwuJiIhIkmu2s2xEREQkcTSFLhslJCIiIklOg1pFREQkcGohERERkcBpDImIiIgETiu1ioiISODUQiIiIiKB0xgSERERCZy6bERERCRwaiERERGRwCkhERERkcAlfzoC1hSyqqbCzMa7++Sg45CmQc+THGx6piSe9LbfxDI+6ACkSdHzJAebnimJGyUkIiIiEjglJCIiIhI4JSSJRX2zcjDpeZKDTc+UxI0GtYqIiEjg1EIiIiIigVNCkgDM7AkzyzOzJUHHIsnPzHqb2Vtm9omZLTWzW4KOSZKbmbUys/lm9lHsmfpl0DFJ06MumwRgZqcDu4Cn3f2YoOOR5GZm3YHu7v6BmbUHFgJj3H1ZwKFJkjIzA9q6+y4zSwXeAW5x93kBhyZNiFpIEoC7zwEKgo5DmgZ33+juH8T2dwKfAD2DjUqSmZfaFTtMjW36bVYOKiUkIk2YmfUFTgTeDzgUSXJmFjazRUAeMNPd9UzJQaWERKSJMrN2wH+BW919R9DxSHJz94i7nwD0AoaZmbqX5aBSQiLSBMX6+f8L/NPd/xd0PNJ0uPs2YBYwKthIpKlRQiLSxMQGIP4V+MTdfx90PJL8zKyrmXWK7bcGzgKWBxqUNDlKSBKAmf0LeA84wsyyzew7QcckSW04cCVwppktim3nBx2UJLXuwFtm9jGwgNIxJC8HHJM0MZr2KyIiIoFTC4mIiIgETgmJiIiIBE4JiYiIiAROCYmIiIgETgmJiIiIBE4JiYiIiAROCYmIiIgETgmJiIiIBO7/Aww/17Rptnx8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplots(figsize = (10, 5))\n",
    "sns.heatmap(evaluation_Sharpe, annot =True, fmt=\".2f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "So our heatmap gives us some interestig findings:\n",
    "\n",
    "* Holding LAYERS constant, the Sharpe Ratio for different WINDOWS doesn't seem to have some pattern. When LAYER=1, the Sharpe Ratio is just reverting around while LAYERS=2, it seems to reach the highest value at WINDOWS=40 and begin to decrease. But for LAYERS at 3, we can see that the highest ratio is 0.61 when WINDOWS=50.\n",
    "\n",
    "\n",
    "* Holding WINDOWS constant, overall speaking, the Sharpe Ratio is higher for larger number of LAYERS. But we can still see some abnormal outcome.\n",
    "\n",
    "\n",
    "* In conlusion, we will choose WINDOWS=50 and LAYERS=3 to be our best model using the whole data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_1 = 50\n",
    "L_1 = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Evaluate model performance for the ⅔ of the data choose optimal parameters/hyper-parameters using ⅔ of data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1/1 [==============================] - 8s 8s/step - loss: 0.0132\n",
      "Epoch 2/15\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0029\n",
      "Epoch 3/15\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0055\n",
      "Epoch 4/15\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0056\n",
      "Epoch 5/15\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0030\n",
      "Epoch 6/15\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0026\n",
      "Epoch 7/15\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0033\n",
      "Epoch 8/15\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0039\n",
      "Epoch 9/15\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0047\n",
      "Epoch 10/15\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.0024\n",
      "Epoch 11/15\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0027\n",
      "Epoch 12/15\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0024\n",
      "Epoch 13/15\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0027\n",
      "Epoch 14/15\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0033\n",
      "Epoch 15/15\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0029\n",
      "WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f93553ad5e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "asset = Data_sect1.copy()\n",
    "rnn = my_RNN(asset, W_1, L_1)\n",
    "\n",
    "# Two thirds of the data\n",
    "twothirds = pd.concat([Data_sect2, Data_sect3])\n",
    "asset = twothirds.copy()\n",
    "\n",
    "# Set random state\n",
    "random.seed(442)\n",
    "seed(442)\n",
    "tf.random.set_seed(442)\n",
    "        \n",
    "(input_sequences, y) = sequentialize(asset)\n",
    "R = evaluate(asset, rnn, input_sequences, WINDOW)\n",
    "Sharpe = sharpe_ratio(pd.DataFrame(R), monthly_risk_free_rate)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Sample Sharpe Ratio :  0.6063715869590461\n",
      "Out of Sample Sharpe Ratio :  -0.33198408858560313\n"
     ]
    }
   ],
   "source": [
    "print(\"In Sample Sharpe Ratio : \", evaluation_Sharpe.max().max())\n",
    "print(\"Out of Sample Sharpe Ratio : \", Sharpe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We chose the optimal hyperparameters from the whole data and apply it to the first one-third of the data and come up with the model. Then use the model to evaluate the other two-thirds of the data.\n",
    "\n",
    "But it seems is not performing well on the two-thirds part of our data because of the negative value of Sharpe Ratio. It indicates under our model set, this part of data doesn't return profit higher than market riks free rate with the given model, also indicating that the first one-third of the data is quite different from the other two-thirds part. So we shall try using this part of data to select optimal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_Sharpe2 = pd.DataFrame(np.zeros((len(ts), len(hs))), index = ts, columns = hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "5/5 [==============================] - 15s 27ms/step - loss: 0.0060\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.0033\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0025\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.0026\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0020\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.0020\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0019\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.0017\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0018\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.0021\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.0018\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.0020\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.0018\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.0018\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0019\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 19s 38ms/step - loss: 0.0024\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.0018\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.0018\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.0021\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.0016\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.0016\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.0017\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.0016\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.0018\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.0019\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.0018\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.0019\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.0016\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.0018\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.0018\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 23s 51ms/step - loss: 0.0022\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 0.0019\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 0.0018\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.0021\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.0016\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 0.0017\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 0.0017\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 0.0014\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 0.0017\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 0.0018\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.0019\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.0019\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 0.0016\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.0017\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.0017\n",
      "Epoch 1/15\n",
      "4/4 [==============================] - 15s 55ms/step - loss: 0.0077\n",
      "Epoch 2/15\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.0035\n",
      "Epoch 3/15\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.0028\n",
      "Epoch 4/15\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.0019\n",
      "Epoch 5/15\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.0020\n",
      "Epoch 6/15\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.0026\n",
      "Epoch 7/15\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.0020\n",
      "Epoch 8/15\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.0021\n",
      "Epoch 9/15\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.0019\n",
      "Epoch 10/15\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.0015\n",
      "Epoch 11/15\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.0016\n",
      "Epoch 12/15\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.0020\n",
      "Epoch 13/15\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.0020\n",
      "Epoch 14/15\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.0018\n",
      "Epoch 15/15\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.0016\n",
      "Epoch 1/15\n",
      "4/4 [==============================] - 18s 73ms/step - loss: 0.0026\n",
      "Epoch 2/15\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.0019\n",
      "Epoch 3/15\n",
      "4/4 [==============================] - 0s 67ms/step - loss: 0.0019\n",
      "Epoch 4/15\n",
      "4/4 [==============================] - 0s 67ms/step - loss: 0.0017\n",
      "Epoch 5/15\n",
      "4/4 [==============================] - 0s 67ms/step - loss: 0.0020\n",
      "Epoch 6/15\n",
      "4/4 [==============================] - 0s 67ms/step - loss: 0.0020\n",
      "Epoch 7/15\n",
      "4/4 [==============================] - 0s 67ms/step - loss: 0.0018\n",
      "Epoch 8/15\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0020\n",
      "Epoch 9/15\n",
      "4/4 [==============================] - 0s 64ms/step - loss: 0.0016\n",
      "Epoch 10/15\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0015\n",
      "Epoch 11/15\n",
      "4/4 [==============================] - 0s 67ms/step - loss: 0.0013\n",
      "Epoch 12/15\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0020\n",
      "Epoch 13/15\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0018\n",
      "Epoch 14/15\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0016\n",
      "Epoch 15/15\n",
      "4/4 [==============================] - 0s 65ms/step - loss: 0.0015\n",
      "Epoch 1/15\n",
      "4/4 [==============================] - 23s 91ms/step - loss: 0.0024\n",
      "Epoch 2/15\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 0.0023\n",
      "Epoch 3/15\n",
      "4/4 [==============================] - 0s 81ms/step - loss: 0.0020\n",
      "Epoch 4/15\n",
      "4/4 [==============================] - 0s 81ms/step - loss: 0.0020\n",
      "Epoch 5/15\n",
      "4/4 [==============================] - 0s 78ms/step - loss: 0.0015\n",
      "Epoch 6/15\n",
      "4/4 [==============================] - 0s 80ms/step - loss: 0.0018\n",
      "Epoch 7/15\n",
      "4/4 [==============================] - 0s 80ms/step - loss: 0.0018\n",
      "Epoch 8/15\n",
      "4/4 [==============================] - 0s 76ms/step - loss: 0.0018\n",
      "Epoch 9/15\n",
      "4/4 [==============================] - 0s 81ms/step - loss: 0.0015\n",
      "Epoch 10/15\n",
      "4/4 [==============================] - 0s 81ms/step - loss: 0.0014\n",
      "Epoch 11/15\n",
      "4/4 [==============================] - 0s 80ms/step - loss: 0.0012\n",
      "Epoch 12/15\n",
      "4/4 [==============================] - 0s 81ms/step - loss: 0.0019\n",
      "Epoch 13/15\n",
      "4/4 [==============================] - 0s 83ms/step - loss: 0.0018\n",
      "Epoch 14/15\n",
      "4/4 [==============================] - 0s 83ms/step - loss: 0.0016\n",
      "Epoch 15/15\n",
      "4/4 [==============================] - 0s 81ms/step - loss: 0.0015\n",
      "Epoch 1/15\n",
      "4/4 [==============================] - 14s 78ms/step - loss: 0.0081\n",
      "Epoch 2/15\n",
      "4/4 [==============================] - 0s 72ms/step - loss: 0.0040\n",
      "Epoch 3/15\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.0034\n",
      "Epoch 4/15\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.0031\n",
      "Epoch 5/15\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.0024\n",
      "Epoch 6/15\n",
      "4/4 [==============================] - 0s 70ms/step - loss: 0.0022\n",
      "Epoch 7/15\n",
      "4/4 [==============================] - 0s 67ms/step - loss: 0.0018\n",
      "Epoch 8/15\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.0019\n",
      "Epoch 9/15\n",
      "4/4 [==============================] - 0s 67ms/step - loss: 0.0023\n",
      "Epoch 10/15\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.0019\n",
      "Epoch 11/15\n",
      "4/4 [==============================] - 0s 65ms/step - loss: 0.0016\n",
      "Epoch 12/15\n",
      "4/4 [==============================] - 0s 67ms/step - loss: 0.0023\n",
      "Epoch 13/15\n",
      "4/4 [==============================] - 0s 67ms/step - loss: 0.0016\n",
      "Epoch 14/15\n",
      "4/4 [==============================] - 0s 67ms/step - loss: 0.0020\n",
      "Epoch 15/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 69ms/step - loss: 0.0015\n",
      "Epoch 1/15\n",
      "4/4 [==============================] - 19s 101ms/step - loss: 0.0038\n",
      "Epoch 2/15\n",
      "4/4 [==============================] - 0s 90ms/step - loss: 0.0029\n",
      "Epoch 3/15\n",
      "4/4 [==============================] - 0s 90ms/step - loss: 0.0019\n",
      "Epoch 4/15\n",
      "4/4 [==============================] - 0s 88ms/step - loss: 0.0024\n",
      "Epoch 5/15\n",
      "4/4 [==============================] - 0s 88ms/step - loss: 0.0017\n",
      "Epoch 6/15\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 0.0022\n",
      "Epoch 7/15\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 0.0018\n",
      "Epoch 8/15\n",
      "4/4 [==============================] - 0s 90ms/step - loss: 0.0019\n",
      "Epoch 9/15\n",
      "4/4 [==============================] - 0s 89ms/step - loss: 0.0018\n",
      "Epoch 10/15\n",
      "4/4 [==============================] - 0s 90ms/step - loss: 0.0016\n",
      "Epoch 11/15\n",
      "4/4 [==============================] - 0s 88ms/step - loss: 0.0016\n",
      "Epoch 12/15\n",
      "4/4 [==============================] - 0s 87ms/step - loss: 0.0018\n",
      "Epoch 13/15\n",
      "4/4 [==============================] - 0s 90ms/step - loss: 0.0015\n",
      "Epoch 14/15\n",
      "4/4 [==============================] - 0s 89ms/step - loss: 0.0020\n",
      "Epoch 15/15\n",
      "4/4 [==============================] - 0s 90ms/step - loss: 0.0016\n",
      "Epoch 1/15\n",
      "4/4 [==============================] - 24s 124ms/step - loss: 0.0037\n",
      "Epoch 2/15\n",
      "4/4 [==============================] - 0s 116ms/step - loss: 0.0031\n",
      "Epoch 3/15\n",
      "4/4 [==============================] - 0s 115ms/step - loss: 0.0024\n",
      "Epoch 4/15\n",
      "4/4 [==============================] - 0s 118ms/step - loss: 0.0018\n",
      "Epoch 5/15\n",
      "4/4 [==============================] - 0s 116ms/step - loss: 0.0019\n",
      "Epoch 6/15\n",
      "4/4 [==============================] - 0s 115ms/step - loss: 0.0018\n",
      "Epoch 7/15\n",
      "4/4 [==============================] - 0s 112ms/step - loss: 0.0016\n",
      "Epoch 8/15\n",
      "4/4 [==============================] - 0s 115ms/step - loss: 0.0017\n",
      "Epoch 9/15\n",
      "4/4 [==============================] - 0s 116ms/step - loss: 0.0018\n",
      "Epoch 10/15\n",
      "4/4 [==============================] - 0s 116ms/step - loss: 0.0015\n",
      "Epoch 11/15\n",
      "4/4 [==============================] - 0s 112ms/step - loss: 0.0015\n",
      "Epoch 12/15\n",
      "4/4 [==============================] - 0s 116ms/step - loss: 0.0017\n",
      "Epoch 13/15\n",
      "4/4 [==============================] - 0s 109ms/step - loss: 0.0016\n",
      "Epoch 14/15\n",
      "4/4 [==============================] - 0s 117ms/step - loss: 0.0018\n",
      "Epoch 15/15\n",
      "4/4 [==============================] - 0s 116ms/step - loss: 0.0016\n",
      "Epoch 1/15\n",
      "4/4 [==============================] - 14s 98ms/step - loss: 0.0084\n",
      "Epoch 2/15\n",
      "4/4 [==============================] - 0s 91ms/step - loss: 0.0041\n",
      "Epoch 3/15\n",
      "4/4 [==============================] - 0s 88ms/step - loss: 0.0033\n",
      "Epoch 4/15\n",
      "4/4 [==============================] - 0s 90ms/step - loss: 0.0024\n",
      "Epoch 5/15\n",
      "4/4 [==============================] - 0s 89ms/step - loss: 0.0022\n",
      "Epoch 6/15\n",
      "4/4 [==============================] - 0s 89ms/step - loss: 0.0018\n",
      "Epoch 7/15\n",
      "4/4 [==============================] - 0s 88ms/step - loss: 0.0021\n",
      "Epoch 8/15\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 0.0016\n",
      "Epoch 9/15\n",
      "4/4 [==============================] - 0s 89ms/step - loss: 0.0020\n",
      "Epoch 10/15\n",
      "4/4 [==============================] - 0s 89ms/step - loss: 0.0020\n",
      "Epoch 11/15\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 0.0019\n",
      "Epoch 12/15\n",
      "4/4 [==============================] - 0s 91ms/step - loss: 0.0013\n",
      "Epoch 13/15\n",
      "4/4 [==============================] - 0s 86ms/step - loss: 0.0017\n",
      "Epoch 14/15\n",
      "4/4 [==============================] - 0s 89ms/step - loss: 0.0016\n",
      "Epoch 15/15\n",
      "4/4 [==============================] - 0s 88ms/step - loss: 0.0016\n",
      "Epoch 1/15\n",
      "4/4 [==============================] - 19s 131ms/step - loss: 0.0036\n",
      "Epoch 2/15\n",
      "4/4 [==============================] - 0s 122ms/step - loss: 0.0021\n",
      "Epoch 3/15\n",
      "4/4 [==============================] - 0s 113ms/step - loss: 0.0019\n",
      "Epoch 4/15\n",
      "4/4 [==============================] - 0s 114ms/step - loss: 0.0019\n",
      "Epoch 5/15\n",
      "4/4 [==============================] - 0s 117ms/step - loss: 0.0020\n",
      "Epoch 6/15\n",
      "4/4 [==============================] - 0s 117ms/step - loss: 0.0019\n",
      "Epoch 7/15\n",
      "4/4 [==============================] - 0s 122ms/step - loss: 0.0016\n",
      "Epoch 8/15\n",
      "4/4 [==============================] - 0s 117ms/step - loss: 0.0014\n",
      "Epoch 9/15\n",
      "4/4 [==============================] - 0s 119ms/step - loss: 0.0016\n",
      "Epoch 10/15\n",
      "4/4 [==============================] - 0s 116ms/step - loss: 0.0017\n",
      "Epoch 11/15\n",
      "4/4 [==============================] - 0s 118ms/step - loss: 0.0017\n",
      "Epoch 12/15\n",
      "4/4 [==============================] - 0s 117ms/step - loss: 0.0015\n",
      "Epoch 13/15\n",
      "4/4 [==============================] - 0s 114ms/step - loss: 0.0016\n",
      "Epoch 14/15\n",
      "4/4 [==============================] - 0s 113ms/step - loss: 0.0015\n",
      "Epoch 15/15\n",
      "4/4 [==============================] - 0s 118ms/step - loss: 0.0016\n",
      "Epoch 1/15\n",
      "4/4 [==============================] - 24s 164ms/step - loss: 0.0039\n",
      "Epoch 2/15\n",
      "4/4 [==============================] - 1s 141ms/step - loss: 0.0033\n",
      "Epoch 3/15\n",
      "4/4 [==============================] - 1s 141ms/step - loss: 0.0024\n",
      "Epoch 4/15\n",
      "4/4 [==============================] - 1s 149ms/step - loss: 0.0016\n",
      "Epoch 5/15\n",
      "4/4 [==============================] - 1s 143ms/step - loss: 0.0023\n",
      "Epoch 6/15\n",
      "4/4 [==============================] - 1s 135ms/step - loss: 0.0018\n",
      "Epoch 7/15\n",
      "4/4 [==============================] - 1s 143ms/step - loss: 0.0016\n",
      "Epoch 8/15\n",
      "4/4 [==============================] - 1s 129ms/step - loss: 0.0014\n",
      "Epoch 9/15\n",
      "4/4 [==============================] - 1s 137ms/step - loss: 0.0016\n",
      "Epoch 10/15\n",
      "4/4 [==============================] - 1s 141ms/step - loss: 0.0017\n",
      "Epoch 11/15\n",
      "4/4 [==============================] - 1s 144ms/step - loss: 0.0017\n",
      "Epoch 12/15\n",
      "4/4 [==============================] - 1s 144ms/step - loss: 0.0015\n",
      "Epoch 13/15\n",
      "4/4 [==============================] - 1s 144ms/step - loss: 0.0015\n",
      "Epoch 14/15\n",
      "4/4 [==============================] - 1s 145ms/step - loss: 0.0014\n",
      "Epoch 15/15\n",
      "4/4 [==============================] - 1s 140ms/step - loss: 0.0014\n",
      "Epoch 1/15\n",
      "3/3 [==============================] - 11s 82ms/step - loss: 0.0068\n",
      "Epoch 2/15\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 0.0041\n",
      "Epoch 3/15\n",
      "3/3 [==============================] - 0s 84ms/step - loss: 0.0034\n",
      "Epoch 4/15\n",
      "3/3 [==============================] - 0s 83ms/step - loss: 0.0031\n",
      "Epoch 5/15\n",
      "3/3 [==============================] - 0s 81ms/step - loss: 0.0024\n",
      "Epoch 6/15\n",
      "3/3 [==============================] - 0s 106ms/step - loss: 0.0022\n",
      "Epoch 7/15\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 0.0020\n",
      "Epoch 8/15\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 0.0022\n",
      "Epoch 9/15\n",
      "3/3 [==============================] - 0s 84ms/step - loss: 0.0023\n",
      "Epoch 10/15\n",
      "3/3 [==============================] - 0s 89ms/step - loss: 0.0019\n",
      "Epoch 11/15\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 0.0018\n",
      "Epoch 12/15\n",
      "3/3 [==============================] - 0s 83ms/step - loss: 0.0015\n",
      "Epoch 13/15\n",
      "3/3 [==============================] - 0s 89ms/step - loss: 0.0022\n",
      "Epoch 14/15\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 0.0019\n",
      "Epoch 15/15\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 0.0020\n",
      "Epoch 1/15\n",
      "3/3 [==============================] - 15s 109ms/step - loss: 0.0037\n",
      "Epoch 2/15\n",
      "3/3 [==============================] - 0s 106ms/step - loss: 0.0028\n",
      "Epoch 3/15\n",
      "3/3 [==============================] - 0s 121ms/step - loss: 0.0022\n",
      "Epoch 4/15\n",
      "3/3 [==============================] - 0s 118ms/step - loss: 0.0024\n",
      "Epoch 5/15\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 0.0018\n",
      "Epoch 6/15\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 0.0019\n",
      "Epoch 7/15\n",
      "3/3 [==============================] - 0s 116ms/step - loss: 0.0019\n",
      "Epoch 8/15\n",
      "3/3 [==============================] - 0s 104ms/step - loss: 0.0017\n",
      "Epoch 9/15\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 0.0017\n",
      "Epoch 10/15\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 0.0018\n",
      "Epoch 11/15\n",
      "3/3 [==============================] - 0s 104ms/step - loss: 0.0016\n",
      "Epoch 12/15\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 0.0015\n",
      "Epoch 13/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 108ms/step - loss: 0.0018\n",
      "Epoch 14/15\n",
      "3/3 [==============================] - 0s 113ms/step - loss: 0.0016\n",
      "Epoch 15/15\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 0.0018\n",
      "Epoch 1/15\n",
      "3/3 [==============================] - 9s 34ms/step - loss: 0.0037\n",
      "Epoch 2/15\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.0042\n",
      "Epoch 3/15\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.0017\n",
      "Epoch 4/15\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.0019\n",
      "Epoch 5/15\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.0016\n",
      "Epoch 6/15\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.0021\n",
      "Epoch 7/15\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.0017\n",
      "Epoch 8/15\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.0019\n",
      "Epoch 9/15\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.0016\n",
      "Epoch 10/15\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.0018\n",
      "Epoch 11/15\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.0015\n",
      "Epoch 12/15\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.0017\n",
      "Epoch 13/15\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.0018\n",
      "Epoch 14/15\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.0015\n",
      "Epoch 15/15\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.0018\n"
     ]
    }
   ],
   "source": [
    "twothirds = pd.concat([Data_sect2, Data_sect3])\n",
    "asset = twothirds.copy()\n",
    "monthly_risk_free_rate = 1.01**(1/12)-1\n",
    "\n",
    "for n, WINDOW in enumerate(ts):\n",
    "    for m , LAYERS in enumerate(hs):\n",
    "        asset = twothirds.copy()\n",
    "        \n",
    "        # Set random state\n",
    "        random.seed(442)\n",
    "        seed(442)\n",
    "        tf.random.set_seed(442)\n",
    "        \n",
    "        rnn = my_RNN(asset, WINDOW, LAYERS)\n",
    "        (input_sequences, y ) = sequentialize(asset)\n",
    "        R = evaluate(asset, rnn, input_sequences, WINDOW)\n",
    "        Sharpe = sharpe_ratio(pd.DataFrame(R), monthly_risk_free_rate)[0]\n",
    "        evaluation_Sharpe2.iloc[n,m] = Sharpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplots(figsize = (10, 5))\n",
    "sns.heatmap(evaluation_Sharpe2, annot=True, fmt=\".2f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very similar results irrespective of hyperparameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
